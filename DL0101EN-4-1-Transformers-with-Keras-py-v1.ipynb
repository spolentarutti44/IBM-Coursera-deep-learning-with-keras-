{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://cognitiveclass.ai\"><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0101EN-SkillsNetwork/images/IDSN-logo.png\" width=\"400\"> </a>\n",
    "\n",
    "# Transformers with Keras\n",
    "\n",
    "Estimated time needed **45** mins\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we will learn how to use the Keras library to build a transformer using a sequence-to-sequence architecture with self-attention for translation. We will train the model using a sample dataset and then use this model for English to Spanish translation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objectives for this Notebook    \n",
    "* How to use the Keras library to build transformers model\n",
    "* Train the transformer model using a given dataset\n",
    "* Use the trained transformer model to translate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Table of Contents</h2>\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "\n",
    "<font size = 4>\n",
    "1. <a href=\"#Import-Keras-and-Packages\">Import Keras and Packages</a><br>\n",
    "2. <a href=\"#Step-1:-Data-Preparation\">Step 1: Data Preparation</a><br>\n",
    "3. <a href=\"#Step-2:-Self-Attention-Layer\">Step 2: Self-Attention Layer</a><br>\n",
    "4. <a href=\"#Step-3:-Model-Architecture\">Step 3: Model Architecture</a><br>\n",
    "5. <a href=\"#Step-4:-Training-the-Model\">Step 4: Training the Model</a><br>\n",
    "6. <a href=\"#Step-5:-Plotting-the-training-loss\">Step 5: Plotting the training loss</a><br>\n",
    "\n",
    "</font>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Keras and Packages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by importing the keras libraries and the packages that we would need to build a neural network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following required libraries are __not__ pre-installed in the Skills Network Labs environment. __You will need to run the following cell__ to install them:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow==2.17.1 in /opt/conda/lib/python3.12/site-packages (2.17.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.17.1) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.17.1) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.17.1) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.17.1) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.17.1) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.17.1) (3.14.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.17.1) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.17.1) (0.4.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.17.1) (3.4.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.17.1) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.17.1) (4.25.8)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.17.1) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.17.1) (75.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.17.1) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.17.1) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.17.1) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.17.1) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.17.1) (1.73.1)\n",
      "Requirement already satisfied: tensorboard<2.18,>=2.17 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.17.1) (2.17.1)\n",
      "Requirement already satisfied: keras>=3.2.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.17.1) (3.10.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.17.1) (1.26.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow==2.17.1) (0.45.1)\n",
      "Requirement already satisfied: rich in /opt/conda/lib/python3.12/site-packages (from keras>=3.2.0->tensorflow==2.17.1) (14.0.0)\n",
      "Requirement already satisfied: namex in /opt/conda/lib/python3.12/site-packages (from keras>=3.2.0->tensorflow==2.17.1) (0.1.0)\n",
      "Requirement already satisfied: optree in /opt/conda/lib/python3.12/site-packages (from keras>=3.2.0->tensorflow==2.17.1) (0.16.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow==2.17.1) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow==2.17.1) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow==2.17.1) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow==2.17.1) (2024.12.14)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.12/site-packages (from tensorboard<2.18,>=2.17->tensorflow==2.17.1) (3.8.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.12/site-packages (from tensorboard<2.18,>=2.17->tensorflow==2.17.1) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.12/site-packages (from tensorboard<2.18,>=2.17->tensorflow==2.17.1) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow==2.17.1) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.12/site-packages (from rich->keras>=3.2.0->tensorflow==2.17.1) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.12/site-packages (from rich->keras>=3.2.0->tensorflow==2.17.1) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow==2.17.1) (0.1.2)\n",
      "Requirement already satisfied: matplotlib==3.9.2 in /opt/conda/lib/python3.12/site-packages (3.9.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.12/site-packages (from matplotlib==3.9.2) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.12/site-packages (from matplotlib==3.9.2) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.12/site-packages (from matplotlib==3.9.2) (4.59.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.12/site-packages (from matplotlib==3.9.2) (1.4.8)\n",
      "Requirement already satisfied: numpy>=1.23 in /opt/conda/lib/python3.12/site-packages (from matplotlib==3.9.2) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.12/site-packages (from matplotlib==3.9.2) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.12/site-packages (from matplotlib==3.9.2) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.12/site-packages (from matplotlib==3.9.2) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.12/site-packages (from matplotlib==3.9.2) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib==3.9.2) (1.17.0)\n",
      "==== All required libraries are installed =====\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==2.17.1\n",
    "!pip install matplotlib==3.9.2\n",
    "\n",
    "print(\"==== All required libraries are installed =====\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Suppress the tensorflow warning messages\n",
    "We use the following code to  suppress the warning messages due to use of CPU architechture for tensoflow.\n",
    "\n",
    "You may want to **comment out** these lines if you are using the GPU architechture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### To use Keras, you will also need to install a backend framework – such as TensorFlow.\n",
    "\n",
    "If you install TensorFlow 2.16 or above, it will install Keras by default.\n",
    "\n",
    "We are using the CPU version of tensorflow since we are dealing with smaller datasets. \n",
    "You may install the GPU version of tensorflow on your machine to accelarate the processing of larger datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the necessary libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, Embedding, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import backend as K\n",
    "from keras.layers import Layer\n",
    "import warnings\n",
    "warnings.simplefilter('ignore',FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, Embedding, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import backend as K\n",
    "from keras.layers import Layer\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Data Preparation\n",
    "We start by define the sentences and text for translation training\n",
    "Sentence Pairs: Defines a small dataset of English-Spanish sentence pairs.\n",
    "Target Sequences:\n",
    "Prepends \"startseq\" and appends \"endseq\" to each target sentence for the decoder to learn when to start and stop translating.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample parallel sentences (English -> Spanish)\n",
    "input_texts = [\n",
    "    \"Hello.\", \"How are you?\", \"I am learning machine translation.\", \"What is your name?\", \"I love programming.\"\n",
    "]\n",
    "target_texts = [\n",
    "    \"Hola.\", \"¿Cómo estás?\", \"Estoy aprendiendo traducción automática.\", \"¿Cuál es tu nombre?\", \"Me encanta programar.\"\n",
    "]\n",
    "\n",
    "target_texts = [\"startseq \" + x + \" endseq\" for x in target_texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next, we convert the text from the sentences to tokens and create a vocabulary\n",
    "Tokenization: Uses Tokenizer to convert words into numerical sequences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "input_tokenizer = Tokenizer()\n",
    "input_tokenizer.fit_on_texts(input_texts)\n",
    "input_sequences = input_tokenizer.texts_to_sequences(input_texts)\n",
    "\n",
    "output_tokenizer = Tokenizer()\n",
    "output_tokenizer.fit_on_texts(target_texts)\n",
    "output_sequences = output_tokenizer.texts_to_sequences(target_texts)\n",
    "\n",
    "input_vocab_size = len(input_tokenizer.word_index) + 1\n",
    "output_vocab_size = len(output_tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now pad the corresponding sentences\n",
    "Padding: Ensures all sequences have the same length.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2  0  0  0  0]\n",
      " [ 3  4  5  0  0]\n",
      " [ 1  6  7  8  9]\n",
      " [10 11 12 13  0]\n",
      " [ 1 14 15  0  0]]\n",
      "[[ 1  3  2  0  0  0]\n",
      " [ 1  4  5  2  0  0]\n",
      " [ 1  6  7  8  9  2]\n",
      " [ 1 10 11 12 13  2]\n",
      " [ 1 14 15 16  2  0]]\n"
     ]
    }
   ],
   "source": [
    "# Padding\n",
    "max_input_length = max([len(seq) for seq in input_sequences])\n",
    "max_output_length = max([len(seq) for seq in output_sequences])\n",
    "\n",
    "input_sequences = pad_sequences(input_sequences, maxlen=max_input_length, padding='post')\n",
    "output_sequences = pad_sequences(output_sequences, maxlen=max_output_length, padding='post')\n",
    "\n",
    "print(input_sequences)\n",
    "print(output_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "# Prepare the target data for training\n",
    "decoder_input_data = output_sequences[:, :-1]\n",
    "decoder_output_data = output_sequences[:, 1:]\n",
    "\n",
    "# Convert to one-hot\n",
    "decoder_output_data = np.array([np.eye(output_vocab_size)[seq] for seq in decoder_output_data])\n",
    "print(decoder_output_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Self-Attention Layer\n",
    "Self-attention is a mechanism that allows a model to **focus on relevant parts of the input sequence** while processing each word. This is particularly useful in:\n",
    "1) Machine Translation (e.g., aligning words correctly)\n",
    "2) Text Summarization\n",
    "3) Speech Recognition\n",
    "4) Image Processing (Vision Transformers)\n",
    "In this implementation, self-attention is used for text based sequence-to-sequence modeling.\n",
    "\n",
    "\n",
    "Self-Attention works for a given an input sequence by computing a weighted representation of all words for each position. It does so using three key components:\n",
    "\n",
    "1. Query **(Q)**, Key **(K)**, and Value **(V)** Matrices\n",
    "For each word (token) in a sequence:\n",
    "\n",
    "Query (Q): What this word is looking for.\n",
    "Key (K): What this word represents.\n",
    "Value (V): The actual information in the word.\n",
    "\n",
    "2. Compute **Attention Scores**\n",
    "Next, we **calculate the similarity between each query and key** using dot-product attention:\n",
    "Each word in a sequence attends to every other word based on these scores.\n",
    "\n",
    "3. Apply **Scaling & Softmax**\n",
    "Since dot-product values can be large, we scale them. \n",
    "Next, Applying softmax converts scores into attention weights:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-Attention class\n",
    "In this implementation of self-attention layer:\n",
    "1. We first initialize the weights in the **build** method, where:\n",
    "    1. **self.Wq**, **self.Wk**, **self.Wv** are the trainable weight matrices.\n",
    "    2. Their **shape is (feature_dim, feature_dim)**, meaning they transform input features into Q, K, and V representations.\n",
    "2. Applying Attention using **call** method. The **call()** method:\n",
    "   1. Computes **Q, K, V** by multiplying inputs (encoder/decoder output) with their respective weight matrices.\n",
    "   2. Computes **dot-product attention scores** using K.batch_dot(q, k, axes=[2, 2]), resulting in a (batch_size, seq_len, seq_len) matrix.\n",
    "   3. **Scales** the scores to avoid large values.\n",
    "   4. Applies **softmax** to normalize the attention scores.\n",
    "   5. **Multiplies attention weights with V** to get the final output.\n",
    "3. The **compute_output_shape** method defines the shape of the output tensor after the layer processes an input.\n",
    "    1. The output shape of the Self-Attention layer **remains the same** as the input shape.\n",
    "    2. The attention mechanism **transforms** the input but does not change its dimensions.4\n",
    "    3. If the attention layer changed the shape, you would modify compute_output_shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Self-Attention Layer\n",
    "class SelfAttention(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(SelfAttention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        feature_dim = input_shape[-1]\n",
    "        # Weight matrices for Q, K, V\n",
    "        self.Wq = self.add_weight(shape=(feature_dim, feature_dim), \n",
    "                                  initializer='glorot_uniform', \n",
    "                                  trainable=True, \n",
    "                                  name='Wq')\n",
    "        self.Wk = self.add_weight(shape=(feature_dim, feature_dim), \n",
    "                                  initializer='glorot_uniform', \n",
    "                                  trainable=True, \n",
    "                                  name='Wk')\n",
    "        self.Wv = self.add_weight(shape=(feature_dim, feature_dim), \n",
    "                                  initializer='glorot_uniform', \n",
    "                                  trainable=True, \n",
    "                                  name='Wv')\n",
    "        super(SelfAttention, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Linear projections\n",
    "        q = K.dot(inputs, self.Wq)  # Query\n",
    "        k = K.dot(inputs, self.Wk)  # Key\n",
    "        v = K.dot(inputs, self.Wv)  # Value\n",
    "\n",
    "        # Scaled Dot-Product Attention\n",
    "        scores = K.batch_dot(q, k, axes=[2, 2])  # (batch, seq_len, seq_len)\n",
    "        scores = scores / K.sqrt(K.cast(K.shape(k)[-1], dtype=K.floatx()))  # Scale\n",
    "        attention_weights = K.softmax(scores, axis=-1)  # Normalize\n",
    "\n",
    "        # Weighted sum of values\n",
    "        output = K.batch_dot(attention_weights, v)  # (batch, seq_len, feature_dim)\n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Model Architecture\n",
    "The model follows an Encoder-Decoder structure:\n",
    "\n",
    "### Encoder:\n",
    "1) Takes input sentences (padded and tokenized).\n",
    "2) Uses an Embedding layer (word representations) + LSTM (to process sequences).\n",
    "    1. The LSTMs are used as the **help process variable-length input sentences** and generate meaningful translations.\n",
    "4) Outputs context vectors (hidden & cell states).\n",
    "\n",
    "### Attention Layer\n",
    "1) Applied to both the encoder and decoder outputs.\n",
    "2) Helps the decoder focus on relevant words during translation.\n",
    "\n",
    "### Decoder\n",
    "1) Receives target sequences (shifted one step ahead).\n",
    "2) Uses an LSTM with encoder states as initial states.\n",
    "3) Applies self-attention for better learning.\n",
    "4) Uses a Dense layer (Softmax) to predict the next word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_8       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_9       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_8         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span> │ input_layer_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_9         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,352</span> │ input_layer_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │ embedding_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),      │            │                   │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]      │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │ embedding_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),      │            │ lstm_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>],     │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]      │            │ lstm_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ self_attention_3    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │    <span style=\"color: #00af00; text-decoration-color: #00af00\">196,608</span> │ lstm_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SelfAttention</span>)     │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>)     │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,369</span> │ self_attention_3… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_8       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_9       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_8         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │      \u001b[38;5;34m4,096\u001b[0m │ input_layer_8[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_9         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │      \u001b[38;5;34m4,352\u001b[0m │ input_layer_9[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_8 (\u001b[38;5;33mLSTM\u001b[0m)       │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m256\u001b[0m),  │    \u001b[38;5;34m525,312\u001b[0m │ embedding_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m),      │            │                   │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)]      │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_9 (\u001b[38;5;33mLSTM\u001b[0m)       │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m256\u001b[0m),  │    \u001b[38;5;34m525,312\u001b[0m │ embedding_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m),      │            │ lstm_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m],     │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)]      │            │ lstm_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m2\u001b[0m]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ self_attention_3    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │    \u001b[38;5;34m196,608\u001b[0m │ lstm_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mSelfAttention\u001b[0m)     │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m17\u001b[0m)     │      \u001b[38;5;34m4,369\u001b[0m │ self_attention_3… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,260,049</span> (4.81 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,260,049\u001b[0m (4.81 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,260,049</span> (4.81 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,260,049\u001b[0m (4.81 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Encoder\n",
    "encoder_inputs = Input(shape=(max_input_length,))\n",
    "encoder_embedding = Embedding(input_vocab_size, 256)(encoder_inputs)\n",
    "encoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Attention Mechanism\n",
    "attention_layer = SelfAttention()(encoder_outputs)\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = Input(shape=(max_output_length - 1,))\n",
    "decoder_embedding = Embedding(output_vocab_size, 256)(decoder_inputs)\n",
    "decoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "decoder_attention = SelfAttention()(decoder_outputs)  # Apply attention\n",
    "decoder_dense = Dense(output_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_attention)\n",
    "\n",
    "# Full Model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Training the Model\n",
    "Uses categorical_crossentropy as the loss function since output words are one-hot encoded.\n",
    "Trains using Adam optimizer for 100 epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step - accuracy: 0.0800 - loss: 2.8303\n",
      "Epoch 2/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step - accuracy: 0.3200 - loss: 2.7966\n",
      "Epoch 3/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step - accuracy: 0.3200 - loss: 2.7565\n",
      "Epoch 4/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 238ms/step - accuracy: 0.2800 - loss: 2.7004\n",
      "Epoch 5/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 210ms/step - accuracy: 0.2800 - loss: 2.6184\n",
      "Epoch 6/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.2800 - loss: 2.5027\n",
      "Epoch 7/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 243ms/step - accuracy: 0.2400 - loss: 2.3656\n",
      "Epoch 8/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 216ms/step - accuracy: 0.2400 - loss: 2.3012\n",
      "Epoch 9/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 204ms/step - accuracy: 0.2800 - loss: 2.3378\n",
      "Epoch 10/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - accuracy: 0.3200 - loss: 2.2768\n",
      "Epoch 11/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 242ms/step - accuracy: 0.3200 - loss: 2.2108\n",
      "Epoch 12/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 214ms/step - accuracy: 0.3200 - loss: 2.1739\n",
      "Epoch 13/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 0.2800 - loss: 2.1444\n",
      "Epoch 14/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178ms/step - accuracy: 0.3200 - loss: 2.1089\n",
      "Epoch 15/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - accuracy: 0.3200 - loss: 2.0615\n",
      "Epoch 16/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.3200 - loss: 2.0049\n",
      "Epoch 17/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.3200 - loss: 1.9507\n",
      "Epoch 18/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - accuracy: 0.3200 - loss: 1.9108\n",
      "Epoch 19/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.3200 - loss: 1.8803\n",
      "Epoch 20/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step - accuracy: 0.3200 - loss: 1.8386\n",
      "Epoch 21/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 0.3200 - loss: 1.7784\n",
      "Epoch 22/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 0.3200 - loss: 1.7121\n",
      "Epoch 23/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 0.3200 - loss: 1.6541\n",
      "Epoch 24/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - accuracy: 0.3200 - loss: 1.6094\n",
      "Epoch 25/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.3200 - loss: 1.5792\n",
      "Epoch 26/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.3200 - loss: 1.5606\n",
      "Epoch 27/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.3200 - loss: 1.5367\n",
      "Epoch 28/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 0.3200 - loss: 1.5154\n",
      "Epoch 29/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.3200 - loss: 1.5157\n",
      "Epoch 30/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.3200 - loss: 1.4995\n",
      "Epoch 31/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.3200 - loss: 1.4726\n",
      "Epoch 32/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 0.3200 - loss: 1.4764\n",
      "Epoch 33/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.3600 - loss: 1.4803\n",
      "Epoch 34/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 0.3200 - loss: 1.4556\n",
      "Epoch 35/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.3200 - loss: 1.4451\n",
      "Epoch 36/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 0.3200 - loss: 1.4448\n",
      "Epoch 37/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.3200 - loss: 1.4283\n",
      "Epoch 38/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step - accuracy: 0.3600 - loss: 1.4202\n",
      "Epoch 39/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - accuracy: 0.3200 - loss: 1.4027\n",
      "Epoch 40/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.3200 - loss: 1.3984\n",
      "Epoch 41/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.3600 - loss: 1.3801\n",
      "Epoch 42/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.3200 - loss: 1.3653\n",
      "Epoch 43/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.3600 - loss: 1.3483\n",
      "Epoch 44/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 0.3600 - loss: 1.3282\n",
      "Epoch 45/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.3600 - loss: 1.3006\n",
      "Epoch 46/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.4800 - loss: 1.2746\n",
      "Epoch 47/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.4800 - loss: 1.2346\n",
      "Epoch 48/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.4400 - loss: 1.1969\n",
      "Epoch 49/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.4400 - loss: 1.1543\n",
      "Epoch 50/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - accuracy: 0.5200 - loss: 1.1078\n",
      "Epoch 51/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 0.5200 - loss: 1.0720\n",
      "Epoch 52/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 0.5600 - loss: 1.0237\n",
      "Epoch 53/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.5600 - loss: 0.9744\n",
      "Epoch 54/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - accuracy: 0.7200 - loss: 0.9278\n",
      "Epoch 55/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - accuracy: 0.6800 - loss: 0.8856\n",
      "Epoch 56/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.6800 - loss: 0.8962\n",
      "Epoch 57/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.6000 - loss: 0.8420\n",
      "Epoch 58/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.6800 - loss: 0.7946\n",
      "Epoch 59/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.6800 - loss: 0.7361\n",
      "Epoch 60/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 0.6400 - loss: 0.7261\n",
      "Epoch 61/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 0.7600 - loss: 0.6688\n",
      "Epoch 62/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 0.8400 - loss: 0.6126\n",
      "Epoch 63/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.8400 - loss: 0.5866\n",
      "Epoch 64/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.8000 - loss: 0.6891\n",
      "Epoch 65/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 0.7600 - loss: 0.7461\n",
      "Epoch 66/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.7600 - loss: 0.7250\n",
      "Epoch 67/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - accuracy: 0.8000 - loss: 0.5161\n",
      "Epoch 68/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step - accuracy: 0.8400 - loss: 0.6927\n",
      "Epoch 69/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.8800 - loss: 0.5243\n",
      "Epoch 70/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.9200 - loss: 0.5409\n",
      "Epoch 71/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.8000 - loss: 0.5919\n",
      "Epoch 72/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.8400 - loss: 0.4752\n",
      "Epoch 73/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 0.8400 - loss: 0.4481\n",
      "Epoch 74/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.8800 - loss: 0.4643\n",
      "Epoch 75/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - accuracy: 0.8400 - loss: 0.4080\n",
      "Epoch 76/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.8800 - loss: 0.3739\n",
      "Epoch 77/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 0.8400 - loss: 0.4107\n",
      "Epoch 78/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.8800 - loss: 0.3651\n",
      "Epoch 79/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 0.9600 - loss: 0.3533\n",
      "Epoch 80/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - accuracy: 0.9200 - loss: 0.3369\n",
      "Epoch 81/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - accuracy: 0.9600 - loss: 0.3119\n",
      "Epoch 82/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 1.0000 - loss: 0.3040\n",
      "Epoch 83/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 0.9200 - loss: 0.2799\n",
      "Epoch 84/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - accuracy: 0.9600 - loss: 0.2686\n",
      "Epoch 85/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.9600 - loss: 0.2584\n",
      "Epoch 86/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 1.0000 - loss: 0.2461\n",
      "Epoch 87/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 1.0000 - loss: 0.2417\n",
      "Epoch 88/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 1.0000 - loss: 0.2293\n",
      "Epoch 89/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 1.0000 - loss: 0.2174\n",
      "Epoch 90/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - accuracy: 1.0000 - loss: 0.2088\n",
      "Epoch 91/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 1.0000 - loss: 0.2006\n",
      "Epoch 92/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 1.0000 - loss: 0.1885\n",
      "Epoch 93/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 1.0000 - loss: 0.1752\n",
      "Epoch 94/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 1.0000 - loss: 0.1649\n",
      "Epoch 95/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 1.0000 - loss: 0.1562\n",
      "Epoch 96/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 1.0000 - loss: 0.1488\n",
      "Epoch 97/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 1.0000 - loss: 0.1405\n",
      "Epoch 98/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 1.0000 - loss: 0.1311\n",
      "Epoch 99/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 1.0000 - loss: 0.1222\n",
      "Epoch 100/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 1.0000 - loss: 0.1139\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Train the Model\n",
    "history_glorot_adam = model.fit([input_sequences, decoder_input_data], decoder_output_data, epochs=100, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Plotting the training loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting training loss\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history_glorot_adam.history['loss'])\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Awesome, now you have succesfully trained a transformers model.\n",
    "### Now let's try some practice excercises\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice excercise 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this practice exercise, let's train the model using \"he_uniform\" initializer instead of \"glorot_uniform\". Then, compare the training loss between model using \"glorot_uniform\" vs \"he_uniform\" initializers by plotting them using matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_4\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_4\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_14      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_15      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_14        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span> │ input_layer_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_15        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,352</span> │ input_layer_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)      │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │ embedding_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),      │            │                   │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]      │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)      │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │ embedding_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),      │            │ lstm_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>],    │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]      │            │ lstm_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ self_attention_9    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │    <span style=\"color: #00af00; text-decoration-color: #00af00\">196,608</span> │ lstm_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SelfAttention</span>)     │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>)     │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,369</span> │ self_attention_9… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_14      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_15      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_14        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │      \u001b[38;5;34m4,096\u001b[0m │ input_layer_14[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_15        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │      \u001b[38;5;34m4,352\u001b[0m │ input_layer_15[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_14 (\u001b[38;5;33mLSTM\u001b[0m)      │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m256\u001b[0m),  │    \u001b[38;5;34m525,312\u001b[0m │ embedding_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m),      │            │                   │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)]      │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_15 (\u001b[38;5;33mLSTM\u001b[0m)      │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m256\u001b[0m),  │    \u001b[38;5;34m525,312\u001b[0m │ embedding_15[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m),      │            │ lstm_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m],    │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)]      │            │ lstm_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m2\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ self_attention_9    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │    \u001b[38;5;34m196,608\u001b[0m │ lstm_15[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "│ (\u001b[38;5;33mSelfAttention\u001b[0m)     │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m17\u001b[0m)     │      \u001b[38;5;34m4,369\u001b[0m │ self_attention_9… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,260,049</span> (4.81 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,260,049\u001b[0m (4.81 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,260,049</span> (4.81 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,260,049\u001b[0m (4.81 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step - accuracy: 0.0800 - loss: 2.8314\n",
      "Epoch 2/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.2800 - loss: 2.7976\n",
      "Epoch 3/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 0.2800 - loss: 2.7572\n",
      "Epoch 4/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 0.2800 - loss: 2.7005\n",
      "Epoch 5/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.2800 - loss: 2.6179\n",
      "Epoch 6/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 0.2800 - loss: 2.5038\n",
      "Epoch 7/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step - accuracy: 0.2800 - loss: 2.3812\n",
      "Epoch 8/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.2800 - loss: 2.3685\n",
      "Epoch 9/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.2800 - loss: 2.3573\n",
      "Epoch 10/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 0.2800 - loss: 2.2720\n",
      "Epoch 11/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - accuracy: 0.3200 - loss: 2.2228\n",
      "Epoch 12/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 0.3200 - loss: 2.2151\n",
      "Epoch 13/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.2000 - loss: 2.2034\n",
      "Epoch 14/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.2000 - loss: 2.1702\n",
      "Epoch 15/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.3200 - loss: 2.1170\n",
      "Epoch 16/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.3200 - loss: 2.0531\n",
      "Epoch 17/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175ms/step - accuracy: 0.3200 - loss: 1.9951\n",
      "Epoch 18/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 0.3200 - loss: 1.9589\n",
      "Epoch 19/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.3200 - loss: 1.9359\n",
      "Epoch 20/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.3200 - loss: 1.8988\n",
      "Epoch 21/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - accuracy: 0.3200 - loss: 1.8440\n",
      "Epoch 22/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 0.3200 - loss: 1.7870\n",
      "Epoch 23/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.3200 - loss: 1.7344\n",
      "Epoch 24/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - accuracy: 0.3200 - loss: 1.6843\n",
      "Epoch 25/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.3200 - loss: 1.6407\n",
      "Epoch 26/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - accuracy: 0.3200 - loss: 1.6055\n",
      "Epoch 27/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 0.3200 - loss: 1.5623\n",
      "Epoch 28/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.3200 - loss: 1.5363\n",
      "Epoch 29/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - accuracy: 0.3200 - loss: 1.5345\n",
      "Epoch 30/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 0.3200 - loss: 1.5051\n",
      "Epoch 31/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.3200 - loss: 1.4764\n",
      "Epoch 32/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 0.3200 - loss: 1.4922\n",
      "Epoch 33/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - accuracy: 0.3200 - loss: 1.4787\n",
      "Epoch 34/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.3200 - loss: 1.4402\n",
      "Epoch 35/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.3200 - loss: 1.4518\n",
      "Epoch 36/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step - accuracy: 0.3600 - loss: 1.4391\n",
      "Epoch 37/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.3200 - loss: 1.4185\n",
      "Epoch 38/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 0.3200 - loss: 1.4253\n",
      "Epoch 39/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 0.3600 - loss: 1.4114\n",
      "Epoch 40/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 0.3200 - loss: 1.3922\n",
      "Epoch 41/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - accuracy: 0.3600 - loss: 1.4011\n",
      "Epoch 42/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - accuracy: 0.3600 - loss: 1.3753\n",
      "Epoch 43/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.3200 - loss: 1.3623\n",
      "Epoch 44/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.3600 - loss: 1.3497\n",
      "Epoch 45/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 0.3600 - loss: 1.3240\n",
      "Epoch 46/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.4000 - loss: 1.3027\n",
      "Epoch 47/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 0.4000 - loss: 1.2733\n",
      "Epoch 48/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.4000 - loss: 1.2409\n",
      "Epoch 49/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 0.4400 - loss: 1.1937\n",
      "Epoch 50/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.5200 - loss: 1.1615\n",
      "Epoch 51/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.5200 - loss: 1.1207\n",
      "Epoch 52/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 0.5200 - loss: 1.0890\n",
      "Epoch 53/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.5600 - loss: 1.0503\n",
      "Epoch 54/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.6000 - loss: 1.0029\n",
      "Epoch 55/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 0.6000 - loss: 0.9635\n",
      "Epoch 56/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.5200 - loss: 0.9265\n",
      "Epoch 57/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.5600 - loss: 0.8763\n",
      "Epoch 58/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.6000 - loss: 0.8794\n",
      "Epoch 59/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 0.5600 - loss: 0.8056\n",
      "Epoch 60/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.6400 - loss: 0.9707\n",
      "Epoch 61/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.7600 - loss: 0.7288\n",
      "Epoch 62/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.6000 - loss: 0.7491\n",
      "Epoch 63/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - accuracy: 0.7200 - loss: 0.7538\n",
      "Epoch 64/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - accuracy: 0.6800 - loss: 0.7614\n",
      "Epoch 65/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - accuracy: 0.6400 - loss: 0.6797\n",
      "Epoch 66/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.8000 - loss: 0.6424\n",
      "Epoch 67/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 0.6800 - loss: 0.6246\n",
      "Epoch 68/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.7200 - loss: 0.6235\n",
      "Epoch 69/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - accuracy: 0.7600 - loss: 0.5927\n",
      "Epoch 70/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.7600 - loss: 0.5668\n",
      "Epoch 71/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - accuracy: 0.8000 - loss: 0.5235\n",
      "Epoch 72/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 0.8000 - loss: 0.5221\n",
      "Epoch 73/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.7600 - loss: 0.5023\n",
      "Epoch 74/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 0.8000 - loss: 0.4877\n",
      "Epoch 75/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - accuracy: 0.8400 - loss: 0.4698\n",
      "Epoch 76/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 0.8400 - loss: 0.4526\n",
      "Epoch 77/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.8400 - loss: 0.4421\n",
      "Epoch 78/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 0.9600 - loss: 0.4201\n",
      "Epoch 79/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.8400 - loss: 0.4094\n",
      "Epoch 80/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.8800 - loss: 0.4001\n",
      "Epoch 81/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - accuracy: 0.8800 - loss: 0.3933\n",
      "Epoch 82/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - accuracy: 0.8800 - loss: 0.3836\n",
      "Epoch 83/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - accuracy: 0.9200 - loss: 0.3688\n",
      "Epoch 84/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - accuracy: 0.9200 - loss: 0.3543\n",
      "Epoch 85/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 0.9200 - loss: 0.3379\n",
      "Epoch 86/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - accuracy: 0.8800 - loss: 0.3223\n",
      "Epoch 87/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.9200 - loss: 0.3097\n",
      "Epoch 88/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 0.9200 - loss: 0.2974\n",
      "Epoch 89/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 0.9200 - loss: 0.2833\n",
      "Epoch 90/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 1.0000 - loss: 0.2704\n",
      "Epoch 91/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 0.9600 - loss: 0.2610\n",
      "Epoch 92/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.9600 - loss: 0.2543\n",
      "Epoch 93/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 0.9600 - loss: 0.2465\n",
      "Epoch 94/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.9600 - loss: 0.2362\n",
      "Epoch 95/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 1.0000 - loss: 0.2286\n",
      "Epoch 96/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 1.0000 - loss: 0.2195\n",
      "Epoch 97/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.9600 - loss: 0.2084\n",
      "Epoch 98/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 0.9600 - loss: 0.1961\n",
      "Epoch 99/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 1.0000 - loss: 0.1842\n",
      "Epoch 100/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 1.0000 - loss: 0.1717\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the Self-Attention Layer\n",
    "class SelfAttentionHe(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(SelfAttentionHe, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        feature_dim = input_shape[-1]\n",
    "        # Weight matrices for Q, K, V\n",
    "        self.Wq = self.add_weight(shape=(feature_dim, feature_dim), \n",
    "                                  initializer='glorot_uniform', \n",
    "                                  trainable=True, \n",
    "                                  name='Wq')\n",
    "        self.Wk = self.add_weight(shape=(feature_dim, feature_dim), \n",
    "                                  initializer='glorot_uniform', \n",
    "                                  trainable=True, \n",
    "                                  name='Wk')\n",
    "        self.Wv = self.add_weight(shape=(feature_dim, feature_dim), \n",
    "                                  initializer='glorot_uniform', \n",
    "                                  trainable=True, \n",
    "                                  name='Wv')\n",
    "        super(SelfAttentionHe, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Linear projections\n",
    "        q = K.dot(inputs, self.Wq)  # Query\n",
    "        k = K.dot(inputs, self.Wk)  # Key\n",
    "        v = K.dot(inputs, self.Wv)  # Value\n",
    "\n",
    "        # Scaled Dot-Product Attention\n",
    "        scores = K.batch_dot(q, k, axes=[2, 2])  # (batch, seq_len, seq_len)\n",
    "        scores = scores / K.sqrt(K.cast(K.shape(k)[-1], dtype=K.floatx()))  # Scale\n",
    "        attention_weights = K.softmax(scores, axis=-1)  # Normalize\n",
    "\n",
    "        # Weighted sum of values\n",
    "        output = K.batch_dot(attention_weights, v)  # (batch, seq_len, feature_dim)\n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "        \n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(max_input_length,))\n",
    "encoder_embedding = Embedding(input_vocab_size, 256)(encoder_inputs)\n",
    "encoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Attention Mechanism\n",
    "attention_layer = SelfAttention()(encoder_outputs)\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = Input(shape=(max_output_length - 1,))\n",
    "decoder_embedding = Embedding(output_vocab_size, 256)(decoder_inputs)\n",
    "decoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "decoder_attention = SelfAttention()(decoder_outputs)  # Apply attention\n",
    "decoder_dense = Dense(output_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_attention)\n",
    "\n",
    "# Full Model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Summary\n",
    "#model.summary()\n",
    "        \n",
    "\n",
    "history_he_adam = model.fit([input_sequences, decoder_input_data], decoder_output_data, epochs=100, batch_size=16)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history_glorot_adam.history['loss'])\n",
    "plt.plot(history_he_adam.history['loss'], linestyle=\"--\",color=\"green\")\n",
    "plt.title('Traninig loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double-click <b>here</b> for the solution.\n",
    "\n",
    "<!-- Your answer is below:\n",
    "\n",
    "\n",
    "#Define the Self-Attention Layer\n",
    "class SelfAttention(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(SelfAttention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        feature_dim = input_shape[-1]\n",
    "        # Weight matrices for Q, K, V\n",
    "        self.Wq = self.add_weight(shape=(feature_dim, feature_dim), \n",
    "                                  initializer='he_uniform', \n",
    "                                  trainable=True, \n",
    "                                  name='Wq')\n",
    "        self.Wk = self.add_weight(shape=(feature_dim, feature_dim), \n",
    "                                  initializer='he_uniform', \n",
    "                                  trainable=True, \n",
    "                                  name='Wk')\n",
    "        self.Wv = self.add_weight(shape=(feature_dim, feature_dim), \n",
    "                                  initializer='he_uniform', \n",
    "                                  trainable=True, \n",
    "                                  name='Wv')\n",
    "        super(SelfAttention, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Linear projections\n",
    "        q = K.dot(inputs, self.Wq)  # Query\n",
    "        k = K.dot(inputs, self.Wk)  # Key\n",
    "        v = K.dot(inputs, self.Wv)  # Value\n",
    "\n",
    "        # Scaled Dot-Product Attention\n",
    "        scores = K.batch_dot(q, k, axes=[2, 2])  # (batch, seq_len, seq_len)\n",
    "        scores = scores / K.sqrt(K.cast(K.shape(k)[-1], dtype=K.floatx()))  # Scale\n",
    "        attention_weights = K.softmax(scores, axis=-1)  # Normalize\n",
    "\n",
    "        # Weighted sum of values\n",
    "        output = K.batch_dot(attention_weights, v)  # (batch, seq_len, feature_dim)\n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "    \n",
    "#Encoder\n",
    "encoder_inputs = Input(shape=(max_input_length,))\n",
    "encoder_embedding = Embedding(input_vocab_size, 256)(encoder_inputs)\n",
    "encoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "#Attention Mechanism\n",
    "attention_layer = SelfAttention()(encoder_outputs)\n",
    "\n",
    "#Decoder\n",
    "decoder_inputs = Input(shape=(max_output_length - 1,))\n",
    "decoder_embedding = Embedding(output_vocab_size, 256)(decoder_inputs)\n",
    "decoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "decoder_attention = SelfAttention()(decoder_outputs)  # Apply attention\n",
    "decoder_dense = Dense(output_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_attention)\n",
    "\n",
    "#Full Model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "#Step 6: Train the Model\n",
    "history_he = model.fit([input_sequences, decoder_input_data], decoder_output_data, epochs=100, batch_size=16)\n",
    "\n",
    "#Plotting training losses for glorot_uniform and he_uniform inititalizers\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history_glorot_adam.history['loss'], label=\"glorot_uniform\", color='red')\n",
    "plt.plot(history_he.history['loss'], label=\"he_uniform\", color='blue')\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice excercise 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this practice exercise, try to use adaptive gradient optimizer instead of adam. Then, plot and compare the results between adam and adaptive gradient optimizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_4\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_4\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_14      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_15      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_14        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span> │ input_layer_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_15        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,352</span> │ input_layer_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)      │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │ embedding_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),      │            │                   │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]      │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)      │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │ embedding_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),      │            │ lstm_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>],    │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]      │            │ lstm_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ self_attention_9    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │    <span style=\"color: #00af00; text-decoration-color: #00af00\">196,608</span> │ lstm_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SelfAttention</span>)     │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>)     │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,369</span> │ self_attention_9… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_14      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_15      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_14        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │      \u001b[38;5;34m4,096\u001b[0m │ input_layer_14[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_15        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │      \u001b[38;5;34m4,352\u001b[0m │ input_layer_15[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_14 (\u001b[38;5;33mLSTM\u001b[0m)      │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m256\u001b[0m),  │    \u001b[38;5;34m525,312\u001b[0m │ embedding_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m),      │            │                   │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)]      │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_15 (\u001b[38;5;33mLSTM\u001b[0m)      │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m256\u001b[0m),  │    \u001b[38;5;34m525,312\u001b[0m │ embedding_15[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m),      │            │ lstm_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m],    │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)]      │            │ lstm_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m2\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ self_attention_9    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │    \u001b[38;5;34m196,608\u001b[0m │ lstm_15[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "│ (\u001b[38;5;33mSelfAttention\u001b[0m)     │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m17\u001b[0m)     │      \u001b[38;5;34m4,369\u001b[0m │ self_attention_9… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,260,049</span> (4.81 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,260,049\u001b[0m (4.81 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,260,049</span> (4.81 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,260,049\u001b[0m (4.81 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step - accuracy: 1.0000 - loss: 0.1563\n",
      "Epoch 2/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 1.0000 - loss: 0.1561\n",
      "Epoch 3/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 1.0000 - loss: 0.1559\n",
      "Epoch 4/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 1.0000 - loss: 0.1558\n",
      "Epoch 5/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - accuracy: 1.0000 - loss: 0.1556\n",
      "Epoch 6/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 1.0000 - loss: 0.1554\n",
      "Epoch 7/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 1.0000 - loss: 0.1553\n",
      "Epoch 8/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - accuracy: 1.0000 - loss: 0.1551\n",
      "Epoch 9/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 1.0000 - loss: 0.1550\n",
      "Epoch 10/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 1.0000 - loss: 0.1549\n",
      "Epoch 11/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - accuracy: 1.0000 - loss: 0.1547\n",
      "Epoch 12/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 1.0000 - loss: 0.1546\n",
      "Epoch 13/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 1.0000 - loss: 0.1544\n",
      "Epoch 14/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 1.0000 - loss: 0.1543\n",
      "Epoch 15/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 1.0000 - loss: 0.1542\n",
      "Epoch 16/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - accuracy: 1.0000 - loss: 0.1541\n",
      "Epoch 17/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - accuracy: 1.0000 - loss: 0.1540\n",
      "Epoch 18/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 1.0000 - loss: 0.1538\n",
      "Epoch 19/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - accuracy: 1.0000 - loss: 0.1537\n",
      "Epoch 20/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 1.0000 - loss: 0.1536\n",
      "Epoch 21/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 1.0000 - loss: 0.1535\n",
      "Epoch 22/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - accuracy: 1.0000 - loss: 0.1534\n",
      "Epoch 23/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 1.0000 - loss: 0.1533\n",
      "Epoch 24/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - accuracy: 1.0000 - loss: 0.1532\n",
      "Epoch 25/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 1.0000 - loss: 0.1531\n",
      "Epoch 26/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 1.0000 - loss: 0.1529\n",
      "Epoch 27/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 1.0000 - loss: 0.1528\n",
      "Epoch 28/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 1.0000 - loss: 0.1527\n",
      "Epoch 29/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 1.0000 - loss: 0.1526\n",
      "Epoch 30/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 1.0000 - loss: 0.1525\n",
      "Epoch 31/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 1.0000 - loss: 0.1524\n",
      "Epoch 32/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 1.0000 - loss: 0.1523\n",
      "Epoch 33/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 1.0000 - loss: 0.1522\n",
      "Epoch 34/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 1.0000 - loss: 0.1521"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adagrad', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "history_he_adaptive = model.fit([input_sequences, decoder_input_data], decoder_output_data, epochs=100, batch_size=16)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history_glorot_adam.history['loss'])\n",
    "plt.plot(history_he_adam.history['loss'], linestyle=\"--\",color=\"green\")\n",
    "plt.plot(history_he_adaptive.history['loss'], linestyle=\"-\",color=\"blue\")\n",
    "plt.title('Traninig loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double-click <b>here</b> for the solution.\n",
    "\n",
    "<!-- Your answer is below:\n",
    "\n",
    "#Full Model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='adagrad', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "#Step 6: Train the Model\n",
    "history_adagrad = model.fit([input_sequences, decoder_input_data], decoder_output_data, epochs=100, batch_size=16)\n",
    "\n",
    "#Plotting training losses for glorot_uniform and he_uniform inititalizers\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history_glorot_adam.history['loss'], label=\"adam\", color='red')\n",
    "plt.plot(history_adagrad.history['loss'], label=\"adagrad\", color='blue')\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thank you for completing this lab!\n",
    "\n",
    "This notebook was created by [Aman Aggarwal](https://www.linkedin.com/in/aggarwal-aman/). I hope you found this lab interesting and educational. Feel free to contact me if you have any questions!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "## Change Log\n",
    "\n",
    "|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n",
    "|---|---|---|---|\n",
    "| 2024-11-20  | 1.0  | Aman  |  Created the lab |\n",
    "<hr>\n",
    "-->\n",
    "## <h3 align=\"center\"> © IBM Corporation. All rights reserved. <h3/>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "prev_pub_hash": "89fa9a3db18ab099ea8b241e966f29a2f658cfbd6a742128f10daea40c67df82"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
